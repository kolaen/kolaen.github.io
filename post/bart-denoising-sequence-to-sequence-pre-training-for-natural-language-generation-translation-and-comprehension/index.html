<!DOCTYPE html>
<html>
  <head>
      <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta content="yes" name="apple-mobile-web-app-capable" />
  <meta content="black" name="apple-mobile-web-app-status-bar-style" />
  <meta name="referrer" content="never">
  <meta name="keywords" content="">
  <meta name="description" content="">
  <meta name="author" content="kveln">
  <title>ACL2020 BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension | Gridea</title>
  <link href="https://cdn.bootcss.com/twitter-bootstrap/4.3.1/css/bootstrap.min.css" rel="stylesheet">
  <!-- <link href="http://kolaen.github.io/media/css/bootstrap.min.css" rel="stylesheet"> -->
  <!--  <link href="http://kolaen.github.io/media/css/all.min.css" rel="stylesheet" type="text/css"> -->
  <link href="https://cdn.bootcss.com/font-awesome/5.11.2/css/all.min.css" rel="stylesheet">
  <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
  <link rel="alternate" type="application/rss+xml" title="ACL2020 BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension | Gridea » Feed" href="http://kolaen.github.io/atom.xml">
  <link rel="stylesheet"href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.15.10/build/styles/androidstudio.min.css">
  <link href="http://kolaen.github.io/styles/main.css" rel="stylesheet">
  <script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.15.10/build/highlight.min.js"></script>
  <!-- <script src="http://kolaen.github.io/media/scripts/jquery.min.js"></script> -->
  <script>hljs.initHighlightingOnLoad();</script>
  

    <meta property="og:description" content="ACL2020 BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"/>
    <meta property="og:url" content="http://kolaen.github.io/post/bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension/"/>
    <meta property="og:locale" content="zh-CN"/>
    <meta property="og:type" content="website"/>
    <meta property="og:site_name" content="Gridea"/>
  </head>
  <body>
  	<!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
    <div class="container">
      <a class="navbar-brand" href="http://kolaen.github.io">Gridea</a>
      <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        Menu
        <i class="fas fa-bars"></i>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ml-auto">
          
          <li class="nav-item">
              
              <a class="nav-link" href="/">首页</a>
              
          </li>
          
          <li class="nav-item">
              
              <a class="nav-link" href="/archives">归档</a>
              
          </li>
          
          <li class="nav-item">
              
              <a class="nav-link" href="/tags">标签</a>
              
          </li>
          
          <li class="nav-item">
              
              <a class="nav-link" href="/post/about">关于</a>
              
          </li>
          
        </ul>
      </div>
    </div>
  </nav>
  <!-- Page Header -->
  <header class="masthead" style="background-image: url('http://kolaen.github.io/media/images/home-bg.jpg')">
    <div class="overlay"></div>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <div class="post-heading">
          	<span class="tags">
          	 
        </span>
            <h1>ACL2020 BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</h1>
            <span class="meta">
            	Posted on
              2020-06-10，4 min read
            </span>
          </div>
        </div>
      </div>
    </div>
  </header>

  <!-- Post Content -->
  <article>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <h2 id="1-摘要">1. 摘要</h2>
<p>​		论文提出了一种新的Sequence-to-Sequence预训练模型。模型encoder部分基于BERT（双向encoder），decoder部分基于GPT（单向从左至右decoder），当然还包括了其他的预训练框架。BART训练步骤：（1）添加了任意噪声的混乱文本；（2）模型学习重新构建原始文本。论文的实验发现，使用Sentence Permutation和novel in-filling 方法能够达到最优秀的模型。同时，作者也表示，BART模型对文本生成问题进行微调时，最有效。</p>
<!-- more -->
<h2 id="2-模型">2. 模型</h2>
<p>​		其他方面，除了摘要中提到的优点和结构。作者在进行翻译任务微调时，在BART之前加入了额外的几层Transformer，这几层可以将其他语言的noise翻译成英文noise。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/kolaen/Pic/master/20200610134444.png" alt="12" loading="lazy"></figure>
<h3 id="21-模型结构">2.1 模型结构</h3>
<p>​		结构方面，BART模型使用了标准Seq2Seq Transformer结构然后后接GPT结构。激活函数上，作者使用了改进的ReLU函数——GeLUs。作者使用正态分布<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="script">N</mi><mo>(</mo><mn>0</mn><mo separator="true">,</mo><mn>0.02</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">\mathcal{N}(0,0.02)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.14736em;">N</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">0</span><span class="mord">2</span><span class="mclose">)</span></span></span></span>初始化参数。base模型使用了6层encoder和6层decoder，large模型各使用了12层。</p>
<p>​		BART和BERT模型的不同点：</p>
<ol>
<li>decoder的每一层都增加了一个对encoder的hidden layer的cross-attention机制。</li>
<li>BART没有使用BERT中的feed-forward网络。</li>
</ol>
<h3 id="22-预训练">2.2 预训练</h3>
<p>​		不同于其他只能接受指定类型噪音的方法，任何类型的混乱文本都可以应用到BART上。极端一点说，当文档中的所有信息都丢失时，BART就相当于是一个语言模型了。</p>
<p>​		对于噪声添加或者说叫transformations，作者总结了一下几个方法：</p>
<ol>
<li>Token Masking：和bert中一样，使用&lt;MASK&gt;对单词做替换。</li>
<li>Token Deletion：从输入中随机删除token，这样做使得模型需要作出判断，哪一个位置上的token被删除。</li>
<li>Text Infilling：这是使模型表现最好的一种方式。根据泊松分布<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>λ</mi><mo>=</mo><mn>3</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">(\lambda=3)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault">λ</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">3</span><span class="mclose">)</span></span></span></span>确定span，这个span范围内的token用<strong>一个</strong>&lt;mask&gt;代替。如果span=0，那么就在该位置插入一个&lt;mask&gt;。这种方式教会模型预测多少个token被丢弃。</li>
<li>Sentence Permutation：文档分句后，打乱句子顺序。</li>
<li>Document Rotation：随机均匀地选择一个token，让这个token作为文档的开头的单词。这样可以使模型识别文档的开头。</li>
</ol>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/kolaen/Pic/master/20200610173700.png" alt="" loading="lazy"></figure>
<h3 id="23-模型微调">2.3 模型微调</h3>
<h4 id="231-文本分类">2.3.1 文本分类</h4>
<p>​		BART的两个输入都是相同的分类语句。和BERT不同的是，BART直接使用Decoder最后一层的最后输出作为分类的输入向量。</p>
<h4 id="232-机器翻译">2.3.2 机器翻译</h4>
<p>​		模型结构上，和其他微调模型不同的是，作者将预训练好的BART模型作为新模型的Decoder，然后加入了新的encoder部分。即使用一个多层的Transformer结构，将其他语言的输入转化为英文输入。在训练时，分为两步：（1）固定BART的参数，即不训练BART参数，训练新的encoder的参数；（2）经过少量的迭代，训练整个模型。</p>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/kolaen/Pic/master/20200610175423.png" alt="" loading="lazy"></figure>
<h2 id="3-结果分析">3. 结果分析</h2>
<p>​		总的来说，和其他预训练模型在传统任务上的效果只是有些许的提高。不过既然叫做生成式预训练模型，那肯定是在生成式任务上做的好，比如说自动摘要生成。</p>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/kolaen/Pic/master/20200610175725.png" alt="" loading="lazy"></figure>

          
          <p class="next-post">下一篇：
            <a href="http://kolaen.github.io/post/reinforced-training-data-selection-for-domain-adaptation/">
              <span class="post-title">
                Reinforced Training Data Selection for Domain Adaptation&rarr;
              </span>
            </a>
          </p>
        
        <div class="comment">
          
        </div>
      </div>
    </div>
  </article>
 <!-- Footer -->
  <footer>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <ul class="list-inline text-center">
            
            
              
            
              
            
              
            
              
            
              
            
              
            
              
              <li class="list-inline-item">
              <a href="http://kolaen.github.io/atom.xml" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fas fa-circle fa-stack-2x"></i>
                  <i class="fas fa-rss fa-stack-1x fa-inverse"></i>
                </span>
              </a>
              </li>
          </ul>
          <p class="copyright text-muted">Copyright &copy;<span>Gridea</span><br><a href="https://github.com/getgridea/gridea" class="Themeinfo">Powered by Gridea</a></p>
        </div>
      </div>
    </div>
   </footer>
  <!-- Bootstrap core JavaScript -->
  <script src="https://cdn.bootcss.com/twitter-bootstrap/4.3.1/js/bootstrap.bundle.min.js"></script>
  <!-- <script src="http://kolaen.github.io/media/scripts/bootstrap.bundle.min.js"></script> -->
  <!-- Bootstrap core JavaScript -->
  <script src="https://cdn.jsdelivr.net/gh/Alanrk/clean-cdn@1.0/scripts/clean-blog.min.js"></script>
  <!-- <script src="http://kolaen.github.io/media/scripts/clean-blog.min.js"></script> -->
  <script src="//instant.page/3.0.0" type="module" defer integrity="sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1"></script>
  <style type="text/css">a.back_to_top{text-decoration:none;position:fixed;bottom:40px;right:30px;background:#f0f0f0;height:40px;width:40px;border-radius:50%;line-height:36px;font-size:18px;text-align:center;transition-duration:.5s;transition-propety:background-color;display:none}a.back_to_top span{color:#888}a.back_to_top:hover{cursor:pointer;background:#dfdfdf}a.back_to_top:hover span{color:#555}@media print,screen and(max-width:580px){.back_to_top{display:none!important}}</style>
<a id="back_to_top" href="#" class="back_to_top">
  <span>▲</span></a>
<script>$(document).ready((function(_this) {
    return function() {
      var bt;
      bt = $('#back_to_top');
      if ($(document).width() > 480) {
        $(window).scroll(function() {
          var st;
          st = $(window).scrollTop();
          if (st > 30) {
            return bt.css('display', 'block')
          } else {
            return bt.css('display', 'none')
          }
        });
        return bt.click(function() {
          $('body,html').animate({
            scrollTop: 0
          },
          800);
          return false
        })
      }
    }
  })(this));</script>
  </body>
</html>

