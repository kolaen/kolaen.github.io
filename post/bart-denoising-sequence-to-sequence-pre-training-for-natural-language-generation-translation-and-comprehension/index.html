<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" >

<title>ACL2020 BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension | YimingZ</title>
<meta name="description" content="温故而知新">

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<link rel="shortcut icon" href="https://kolaen.github.io/favicon.ico?v=1605853832996">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="stylesheet" href="https://unpkg.com/papercss@1.6.1/dist/paper.min.css" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
<link rel="stylesheet" href="https://kolaen.github.io/styles/main.css">


<script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>


<link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />


  </head>
  <body>
  
    <nav class="navbar border fixed split-nav">
  <div class="nav-brand">
    <h3><a href="https://kolaen.github.io">YimingZ</a></h3>
  </div>
  <div class="collapsible">
    <input id="collapsible1" type="checkbox" name="collapsible1">
    <button>
      <label for="collapsible1">
        <div class="bar1"></div>
        <div class="bar2"></div>
        <div class="bar3"></div>
      </label>
    </button>
    <div class="collapsible-body">
      <ul class="inline">
        
          <li>
            
              <a href="/" class="menu">
                首页
              </a>
            
          </li>
        
          <li>
            
              <a href="/archives" class="menu">
                归档
              </a>
            
          </li>
        
          <li>
            
              <a href="/tags" class="menu">
                标签
              </a>
            
          </li>
        
          <li>
            
              <a href="/post/about" class="menu">
                关于
              </a>
            
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div id="top" class="row site">
      <div class="sm-12 md-8 col">
        <div class="paper">
          <article class="article">
            <h1>ACL2020 BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</h1>
            <p class="article-meta">
              2020-06-10
              
            </p>
            
            <div class="post-content">
              <h2 id="1-摘要">1. 摘要</h2>
<p>​		论文提出了一种新的Sequence-to-Sequence预训练模型。模型encoder部分基于BERT（双向encoder），decoder部分基于GPT（单向从左至右decoder），当然还包括了其他的预训练框架。BART训练步骤：（1）添加了任意噪声的混乱文本；（2）模型学习重新构建原始文本。论文的实验发现，使用Sentence Permutation和novel in-filling 方法能够达到最优秀的模型。同时，作者也表示，BART模型对文本生成问题进行微调时，最有效。</p>
<!-- more -->
<h2 id="2-模型">2. 模型</h2>
<p>​		其他方面，除了摘要中提到的优点和结构。作者在进行翻译任务微调时，在BART之前加入了额外的几层Transformer，这几层可以将其他语言的noise翻译成英文noise。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/kolaen/Pic/master/20200610134444.png" alt="12" loading="lazy"></figure>
<h3 id="21-模型结构">2.1 模型结构</h3>
<p>​		结构方面，BART模型使用了标准Seq2Seq Transformer结构然后后接GPT结构。激活函数上，作者使用了改进的ReLU函数——GeLUs。作者使用正态分布<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="script">N</mi><mo>(</mo><mn>0</mn><mo separator="true">,</mo><mn>0.02</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">\mathcal{N}(0,0.02)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.14736em;">N</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">0</span><span class="mord">2</span><span class="mclose">)</span></span></span></span>初始化参数。base模型使用了6层encoder和6层decoder，large模型各使用了12层。</p>
<p>​		BART和BERT模型的不同点：</p>
<ol>
<li>decoder的每一层都增加了一个对encoder的hidden layer的cross-attention机制。</li>
<li>BART没有使用BERT中的feed-forward网络。</li>
</ol>
<h3 id="22-预训练">2.2 预训练</h3>
<p>​		不同于其他只能接受指定类型噪音的方法，任何类型的混乱文本都可以应用到BART上。极端一点说，当文档中的所有信息都丢失时，BART就相当于是一个语言模型了。</p>
<p>​		对于噪声添加或者说叫transformations，作者总结了一下几个方法：</p>
<ol>
<li>Token Masking：和bert中一样，使用&lt;MASK&gt;对单词做替换。</li>
<li>Token Deletion：从输入中随机删除token，这样做使得模型需要作出判断，哪一个位置上的token被删除。</li>
<li>Text Infilling：这是使模型表现最好的一种方式。根据泊松分布<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>λ</mi><mo>=</mo><mn>3</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">(\lambda=3)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault">λ</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">3</span><span class="mclose">)</span></span></span></span>确定span，这个span范围内的token用<strong>一个</strong>&lt;mask&gt;代替。如果span=0，那么就在该位置插入一个&lt;mask&gt;。这种方式教会模型预测多少个token被丢弃。</li>
<li>Sentence Permutation：文档分句后，打乱句子顺序。</li>
<li>Document Rotation：随机均匀地选择一个token，让这个token作为文档的开头的单词。这样可以使模型识别文档的开头。</li>
</ol>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/kolaen/Pic/master/20200610173700.png" alt="" loading="lazy"></figure>
<h3 id="23-模型微调">2.3 模型微调</h3>
<h4 id="231-文本分类">2.3.1 文本分类</h4>
<p>​		BART的两个输入都是相同的分类语句。和BERT不同的是，BART直接使用Decoder最后一层的最后输出作为分类的输入向量。</p>
<h4 id="232-机器翻译">2.3.2 机器翻译</h4>
<p>​		模型结构上，和其他微调模型不同的是，作者将预训练好的BART模型作为新模型的Decoder，然后加入了新的encoder部分。即使用一个多层的Transformer结构，将其他语言的输入转化为英文输入。在训练时，分为两步：（1）固定BART的参数，即不训练BART参数，训练新的encoder的参数；（2）经过少量的迭代，训练整个模型。</p>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/kolaen/Pic/master/20200610175423.png" alt="" loading="lazy"></figure>
<h2 id="3-结果分析">3. 结果分析</h2>
<p>​		总的来说，和其他预训练模型在传统任务上的效果只是有些许的提高。不过既然叫做生成式预训练模型，那肯定是在生成式任务上做的好，比如说自动摘要生成。</p>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/kolaen/Pic/master/20200610175725.png" alt="" loading="lazy"></figure>

            </div>
          </article>
        </div>
        <div class="paper" data-aos="fade-in">
          
            <div class="next-post">
              <div class="next">
                下一篇
              </div>
              <a href="https://kolaen.github.io/post/reinforced-training-data-selection-for-domain-adaptation/">
                <h3 class="post-title">
                  Reinforced Training Data Selection for Domain Adaptation
                </h3>
              </a>
            </div>
          
        </div>
        
      </div>

      <div class="sm-12 md-4 col sidebar">
  <div class="paper info-container">
    <img src="https://kolaen.github.io/images/avatar.png?v=1605853832996" class="no-responsive avatar">
    <div class="text-muted">温故而知新</div>
    <div class="social-container">
      
        
      
        
      
        
      
        
      
        
      
    </div>
  </div>
  <div class="paper">
    <div class="sidebar-title">
      最新文章
    </div>
    <div class="row">
      <ul>
        
          
            <li>
              <a href="https://kolaen.github.io/post/ke-neng-hui-fan-de-pytorch-yan-chong-cuo-wu-crossentropyloss-zhi-qian-shi-yong-softmax-ji-huo/">可能会犯的pytorch严重错误：CrossEntropyLoss之前使用Softmax激活</a>
            </li>
          
        
          
            <li>
              <a href="https://kolaen.github.io/post/ddp-cuo-wu-this-error-indicates-that-your-module-has-parameters-that-were-not-used-in-producing-loss/">DDP错误：This error indicates that your module has parameters that were not used in producing loss. </a>
            </li>
          
        
          
            <li>
              <a href="https://kolaen.github.io/post/pytorch-cuo-wu-dang-shi-yong-nccl-zuo-wei-ddp-de-hou-tai-shi-bao-cuo/">pytorch 错误：当使用NCCL作为DDP的后台时报错</a>
            </li>
          
        
          
            <li>
              <a href="https://kolaen.github.io/post/pytorch-cuo-wu-leaf-variable-has-been-moved-into-the-graph-interior/">pytorch错误：leaf variable has been moved into the graph interior </a>
            </li>
          
        
          
            <li>
              <a href="https://kolaen.github.io/post/get-to-the-point-summarization-with-pointer-generator-networks/">Get To The Point: Summarization with Pointer-Generator Networks</a>
            </li>
          
        
          
            <li>
              <a href="https://kolaen.github.io/post/bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension/">ACL2020 BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a>
            </li>
          
        
          
            <li>
              <a href="https://kolaen.github.io/post/reinforced-training-data-selection-for-domain-adaptation/">Reinforced Training Data Selection for Domain Adaptation</a>
            </li>
          
        
          
            <li>
              <a href="https://kolaen.github.io/post/wo-shi-shui/">我是谁？</a>
            </li>
          
        
          
            <li>
              <a href="https://kolaen.github.io/post/jue-ce-shu/">决策树及其特征选择</a>
            </li>
          
        
      </ul>
    </div>
  </div>
  <div class="paper">
    <div class="sidebar-title">
      标签列表
    </div>
    <div class="row">
      
        <a href="https://kolaen.github.io/tag/XAW5i2FPR/" class="badge ">
          pytorch
        </a>
      
        <a href="https://kolaen.github.io/tag/qkUhYl96o/" class="badge success">
          机器学习
        </a>
      
        <a href="https://kolaen.github.io/tag/-wEKMxOLEA/" class="badge success">
          自然语言处理
        </a>
      
        <a href="https://kolaen.github.io/tag/1SDWV8QB9F/" class="badge success">
          域适应
        </a>
      
        <a href="https://kolaen.github.io/tag/HayyR6v2EE/" class="badge secondary">
          统计学习
        </a>
      
    </div>
  </div>
  <div class="paper">
    Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | <a class="rss" href="https://kolaen.github.io/atom.xml" target="_blank">RSS</a>
  </div>
</div>


    </div>

    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>

<script type="application/javascript">

AOS.init();

hljs.initHighlightingOnLoad()

</script>




  </body>
</html>
