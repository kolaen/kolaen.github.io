
<!DOCTYPE html>
<html>
  <head>
    
<meta charset="utf-8" >

<title>Pytorch Distributed Date Parallel Simple Guide | YimingZ</title>
<meta name="description" content="温故而知新">

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.min.css">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="shortcut icon" href="https://kolaen.github.io/favicon.ico?v=1606807137764">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
<link rel="stylesheet" href="https://kolaen.github.io/styles/main.css">



<script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-K2SRM2G1G1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-K2SRM2G1G1');
</script>


  </head>
  <body>
    <div id="app" class="main">
      <div class="site-header-container">
  <div class="site-header">
    <div class="left">
      <a href="https://kolaen.github.io">
        <img class="avatar" src="https://kolaen.github.io/images/avatar.png?v=1606807137764" alt="" width="32px" height="32px">
      </a>
      <a href="https://kolaen.github.io">
        <h1 class="site-title">YimingZ</h1>
      </a>
    </div>
    <div class="right">
      <transition name="fade">
        <i class="icon" :class="{ 'icon-close-outline': menuVisible, 'icon-menu-outline': !menuVisible }" @click="menuVisible = !menuVisible"></i>
      </transition>
    </div>
  </div>
</div>

<transition name="fade">
  <div class="menu-container" style="display: none;" v-show="menuVisible">
    <div class="menu-list">
      
        
          <a href="/" class="menu purple-link">
            首页
          </a>
        
      
        
          <a href="/archives" class="menu purple-link">
            归档
          </a>
        
      
        
          <a href="/tags" class="menu purple-link">
            标签
          </a>
        
      
        
          <a href="/post/about" class="menu purple-link">
            关于
          </a>
        
      
    </div>
  </div>
</transition>


      <div class="content-container">
        <div class="post-detail">
          
          <h2 class="post-title">Pytorch Distributed Date Parallel Simple Guide</h2>
          <div class="post-info post-detail-info">
            <span><i class="icon-calendar-outline"></i> 2020-11-26</span>
            
              <span>
                <i class="icon-pricetags-outline"></i>
                
                  <a href="https://kolaen.github.io/tag/rP34CK1dJ/">
                    DDP
                    
                      ，
                    
                  </a>
                
                  <a href="https://kolaen.github.io/tag/XAW5i2FPR/">
                    pytorch
                    
                  </a>
                
              </span>
            
          </div>
          <div class="post-content">
            <p>总结一下如何使用Pytorch 分布计算神器DDP。本文只介绍并行计算部分，关于DDP的优点其他文章已经进行了详细介绍，我主要总结一下如何快速部署。</p>
<!-- more -->
<h2 id="1-准备工作">1. 准备工作</h2>
<p>使用DDP首先要继承<code>torch.utils.data</code>中的<code>Dataset</code>类，并使用<code>DataLoader</code>加载数据，这样在并行计算时可以避免数据重复的问题。</p>
<p>继承<code>Dataset</code>类的同时也要实现如下的函数。</p>
<pre><code>from torch.utils.data import Dataset

class RelationExtractionDataSet(Dataset):
    def __init__(self,):
        ......

    def __getitem__(self, index):
        ......

    def __len__(self):
        ......
</code></pre>
<h2 id="2-pytorch-ddp">2. Pytorch DDP</h2>
<p>我们以使用最多的<strong>单机多卡</strong>为例，在已有的自定义的<code>DataSet</code>类的前提下，我们在单卡模型的基础上加入如下代码。单机多卡下，setup()和cleanup()都可以直接复用。</p>
<pre><code>import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP


# 初始化
def setup(rank, world_size):
    os.environ['MASTER_ADDR'] = '127.0.0.1'
    os.environ['MASTER_PORT'] = '12355'

    dist.init_process_group(&quot;nccl&quot;, rank=rank, world_size=world_size) 
    torch.cuda.set_device(rank)


# 清空进程
def cleanup():
    dist.destroy_process_group()


# 训练函数
def train(rank, world_size, **kwargs):
    setup(rank, world_size)

    model = MyModel(config)
    model.train()

    dataset = MyDataSet(config)
    data_sampler = DistributedSampler(data_set, world_size, rank)
    data_loader = DataLoader(data_set, batch_size, False, data_sampler)

    optim = AdamW(model.parameters(), lr=lr)

    model = DDP(model, device_ids=[rank], find_unused_parameters=True) 
        # it transforms origin model into DDP model

    for i, batch in enumerate(data_loader):
        loss = model(batch)
        optim.zero_grad()
        loss.backward()
        optim.step()

        if rank == 0:
            print(loss.item())
    
    torch.save(model.module.state_dict(), model_path)
    cleanup()


# 启动多进程程序
def run_demo(demo_fn, world_size, **kwargs):
    mp.spawn(demo_fn,
             args=(world_size, **kwargs,),
             nprocs=world_size,
             join=True)


if __name__ == '__main__':
    run_demo(train, 4, **kwargs)
</code></pre>

          </div>
        </div>

        
          <div class="next-post">
            <a class="purple-link" href="https://kolaen.github.io/post/pytorch-ddp-xian-cun-zhan-yong-bu-jun-heng-wen-ti/">
              <h3 class="post-title">
                下一篇：Pytorch DDP 显存占用不均衡问题
              </h3>
            </a>
          </div>
          
      </div>

      

      <div class="site-footer">
  <div class="slogan">温故而知新</div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | <a class="rss" href="https://kolaen.github.io/atom.xml" target="_blank">RSS</a>
</div>


    </div>
    <script type="application/javascript">

hljs.initHighlightingOnLoad()

var app = new Vue({
  el: '#app',
  data: {
    menuVisible: false,
  },
})

</script>




  </body>
</html>
